{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry \\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\\n        13 is Radius SE, field 23 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': '/home/manikya/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/breast_cancer.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_one_hot = np_utils.to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 5\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y_one_hot,test_size=0.33,random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manikya/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(30,input_dim=30,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2,input_dim=30,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7faf3522ea20>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7faf3522ef28>,\n",
       " <keras.layers.core.Dense at 0x7faf3522ec88>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.summary)\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manikya/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "381/381 [==============================] - 0s 541us/step - loss: 0.7254 - acc: 0.5696\n",
      "Epoch 2/200\n",
      "381/381 [==============================] - 0s 77us/step - loss: 0.6059 - acc: 0.6168\n",
      "Epoch 3/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.5901 - acc: 0.6430\n",
      "Epoch 4/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.5686 - acc: 0.7585\n",
      "Epoch 5/200\n",
      "381/381 [==============================] - 0s 96us/step - loss: 0.5333 - acc: 0.8110\n",
      "Epoch 6/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.5046 - acc: 0.8163\n",
      "Epoch 7/200\n",
      "381/381 [==============================] - 0s 126us/step - loss: 0.4699 - acc: 0.8320\n",
      "Epoch 8/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.4556 - acc: 0.8661\n",
      "Epoch 9/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.4475 - acc: 0.8924\n",
      "Epoch 10/200\n",
      "381/381 [==============================] - 0s 96us/step - loss: 0.4172 - acc: 0.8819\n",
      "Epoch 11/200\n",
      "381/381 [==============================] - 0s 84us/step - loss: 0.3982 - acc: 0.8793\n",
      "Epoch 12/200\n",
      "381/381 [==============================] - 0s 111us/step - loss: 0.3819 - acc: 0.8898\n",
      "Epoch 13/200\n",
      "381/381 [==============================] - 0s 93us/step - loss: 0.3619 - acc: 0.8976\n",
      "Epoch 14/200\n",
      "381/381 [==============================] - 0s 92us/step - loss: 0.3573 - acc: 0.9081\n",
      "Epoch 15/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.3457 - acc: 0.8924\n",
      "Epoch 16/200\n",
      "381/381 [==============================] - 0s 87us/step - loss: 0.3403 - acc: 0.8976\n",
      "Epoch 17/200\n",
      "381/381 [==============================] - 0s 147us/step - loss: 0.3302 - acc: 0.8950\n",
      "Epoch 18/200\n",
      "381/381 [==============================] - 0s 112us/step - loss: 0.3294 - acc: 0.8793\n",
      "Epoch 19/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.3193 - acc: 0.8976\n",
      "Epoch 20/200\n",
      "381/381 [==============================] - 0s 177us/step - loss: 0.3050 - acc: 0.9134\n",
      "Epoch 21/200\n",
      "381/381 [==============================] - 0s 186us/step - loss: 0.3024 - acc: 0.9134\n",
      "Epoch 22/200\n",
      "381/381 [==============================] - 0s 161us/step - loss: 0.3052 - acc: 0.8950\n",
      "Epoch 23/200\n",
      "381/381 [==============================] - 0s 146us/step - loss: 0.3365 - acc: 0.8898\n",
      "Epoch 24/200\n",
      "381/381 [==============================] - 0s 152us/step - loss: 0.2900 - acc: 0.8976\n",
      "Epoch 25/200\n",
      "381/381 [==============================] - 0s 129us/step - loss: 0.2875 - acc: 0.9081\n",
      "Epoch 26/200\n",
      "381/381 [==============================] - 0s 108us/step - loss: 0.2813 - acc: 0.9134\n",
      "Epoch 27/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.2736 - acc: 0.9055\n",
      "Epoch 28/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.2898 - acc: 0.9160\n",
      "Epoch 29/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.2780 - acc: 0.9108\n",
      "Epoch 30/200\n",
      "381/381 [==============================] - 0s 105us/step - loss: 0.2718 - acc: 0.9239\n",
      "Epoch 31/200\n",
      "381/381 [==============================] - 0s 125us/step - loss: 0.2777 - acc: 0.9134\n",
      "Epoch 32/200\n",
      "381/381 [==============================] - 0s 131us/step - loss: 0.2594 - acc: 0.9160\n",
      "Epoch 33/200\n",
      "381/381 [==============================] - 0s 125us/step - loss: 0.2558 - acc: 0.9186\n",
      "Epoch 34/200\n",
      "381/381 [==============================] - 0s 147us/step - loss: 0.2657 - acc: 0.8976\n",
      "Epoch 35/200\n",
      "381/381 [==============================] - 0s 127us/step - loss: 0.2566 - acc: 0.9134\n",
      "Epoch 36/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.2706 - acc: 0.8950\n",
      "Epoch 37/200\n",
      "381/381 [==============================] - 0s 139us/step - loss: 0.2697 - acc: 0.9108\n",
      "Epoch 38/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.2565 - acc: 0.9003\n",
      "Epoch 39/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.2574 - acc: 0.8924\n",
      "Epoch 40/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.2606 - acc: 0.8976\n",
      "Epoch 41/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.2810 - acc: 0.8898\n",
      "Epoch 42/200\n",
      "381/381 [==============================] - 0s 133us/step - loss: 0.2605 - acc: 0.9055\n",
      "Epoch 43/200\n",
      "381/381 [==============================] - 0s 126us/step - loss: 0.2490 - acc: 0.9055\n",
      "Epoch 44/200\n",
      "381/381 [==============================] - 0s 120us/step - loss: 0.2406 - acc: 0.9081\n",
      "Epoch 45/200\n",
      "381/381 [==============================] - 0s 125us/step - loss: 0.2468 - acc: 0.8924\n",
      "Epoch 46/200\n",
      "381/381 [==============================] - 0s 130us/step - loss: 0.2393 - acc: 0.9003\n",
      "Epoch 47/200\n",
      "381/381 [==============================] - 0s 128us/step - loss: 0.2350 - acc: 0.9186\n",
      "Epoch 48/200\n",
      "381/381 [==============================] - 0s 129us/step - loss: 0.2361 - acc: 0.9265\n",
      "Epoch 49/200\n",
      "381/381 [==============================] - 0s 114us/step - loss: 0.2628 - acc: 0.9055\n",
      "Epoch 50/200\n",
      "381/381 [==============================] - 0s 131us/step - loss: 0.2371 - acc: 0.9186\n",
      "Epoch 51/200\n",
      "381/381 [==============================] - 0s 86us/step - loss: 0.2323 - acc: 0.9213\n",
      "Epoch 52/200\n",
      "381/381 [==============================] - 0s 137us/step - loss: 0.2529 - acc: 0.9160\n",
      "Epoch 53/200\n",
      "381/381 [==============================] - 0s 123us/step - loss: 0.2265 - acc: 0.9239\n",
      "Epoch 54/200\n",
      "381/381 [==============================] - 0s 133us/step - loss: 0.2260 - acc: 0.9265\n",
      "Epoch 55/200\n",
      "381/381 [==============================] - 0s 122us/step - loss: 0.2528 - acc: 0.9003\n",
      "Epoch 56/200\n",
      "381/381 [==============================] - 0s 106us/step - loss: 0.2282 - acc: 0.9213\n",
      "Epoch 57/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.2417 - acc: 0.9108\n",
      "Epoch 58/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.2306 - acc: 0.9029\n",
      "Epoch 59/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.2317 - acc: 0.9186\n",
      "Epoch 60/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.2279 - acc: 0.9213\n",
      "Epoch 61/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.2369 - acc: 0.9213\n",
      "Epoch 62/200\n",
      "381/381 [==============================] - 0s 116us/step - loss: 0.2026 - acc: 0.9318\n",
      "Epoch 63/200\n",
      "381/381 [==============================] - 0s 97us/step - loss: 0.2381 - acc: 0.9134\n",
      "Epoch 64/200\n",
      "381/381 [==============================] - 0s 106us/step - loss: 0.2276 - acc: 0.9186\n",
      "Epoch 65/200\n",
      "381/381 [==============================] - 0s 121us/step - loss: 0.2395 - acc: 0.9029\n",
      "Epoch 66/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.2534 - acc: 0.9134\n",
      "Epoch 67/200\n",
      "381/381 [==============================] - 0s 111us/step - loss: 0.3692 - acc: 0.8005\n",
      "Epoch 68/200\n",
      "381/381 [==============================] - 0s 91us/step - loss: 0.2784 - acc: 0.8898\n",
      "Epoch 69/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2491 - acc: 0.8976\n",
      "Epoch 70/200\n",
      "381/381 [==============================] - 0s 112us/step - loss: 0.3129 - acc: 0.8635\n",
      "Epoch 71/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.2219 - acc: 0.9108\n",
      "Epoch 72/200\n",
      "381/381 [==============================] - 0s 91us/step - loss: 0.2094 - acc: 0.9081\n",
      "Epoch 73/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.2162 - acc: 0.9134\n",
      "Epoch 74/200\n",
      "381/381 [==============================] - 0s 93us/step - loss: 0.2289 - acc: 0.9003\n",
      "Epoch 75/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2212 - acc: 0.9213\n",
      "Epoch 76/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.2223 - acc: 0.9160\n",
      "Epoch 77/200\n",
      "381/381 [==============================] - 0s 91us/step - loss: 0.2375 - acc: 0.9081\n",
      "Epoch 78/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.2213 - acc: 0.9081\n",
      "Epoch 79/200\n",
      "381/381 [==============================] - 0s 93us/step - loss: 0.2138 - acc: 0.9003\n",
      "Epoch 80/200\n",
      "381/381 [==============================] - 0s 107us/step - loss: 0.2115 - acc: 0.9081\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/381 [==============================] - 0s 93us/step - loss: 0.2050 - acc: 0.9134\n",
      "Epoch 82/200\n",
      "381/381 [==============================] - 0s 105us/step - loss: 0.2194 - acc: 0.9029\n",
      "Epoch 83/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.2151 - acc: 0.9081\n",
      "Epoch 84/200\n",
      "381/381 [==============================] - 0s 131us/step - loss: 0.1938 - acc: 0.9239\n",
      "Epoch 85/200\n",
      "381/381 [==============================] - 0s 167us/step - loss: 0.2616 - acc: 0.9108\n",
      "Epoch 86/200\n",
      "381/381 [==============================] - 0s 112us/step - loss: 0.2204 - acc: 0.9265\n",
      "Epoch 87/200\n",
      "381/381 [==============================] - 0s 118us/step - loss: 0.2284 - acc: 0.9029\n",
      "Epoch 88/200\n",
      "381/381 [==============================] - 0s 80us/step - loss: 0.2066 - acc: 0.9108\n",
      "Epoch 89/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.2751 - acc: 0.8924\n",
      "Epoch 90/200\n",
      "381/381 [==============================] - 0s 96us/step - loss: 0.2226 - acc: 0.9003\n",
      "Epoch 91/200\n",
      "381/381 [==============================] - 0s 97us/step - loss: 0.2289 - acc: 0.9055\n",
      "Epoch 92/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.2328 - acc: 0.9186\n",
      "Epoch 93/200\n",
      "381/381 [==============================] - 0s 88us/step - loss: 0.2257 - acc: 0.9213\n",
      "Epoch 94/200\n",
      "381/381 [==============================] - 0s 89us/step - loss: 0.2250 - acc: 0.9108\n",
      "Epoch 95/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.2089 - acc: 0.9239\n",
      "Epoch 96/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2264 - acc: 0.9186\n",
      "Epoch 97/200\n",
      "381/381 [==============================] - 0s 84us/step - loss: 0.2065 - acc: 0.9213\n",
      "Epoch 98/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.2121 - acc: 0.9108\n",
      "Epoch 99/200\n",
      "381/381 [==============================] - 0s 88us/step - loss: 0.2142 - acc: 0.9186\n",
      "Epoch 100/200\n",
      "381/381 [==============================] - 0s 84us/step - loss: 0.2192 - acc: 0.9081\n",
      "Epoch 101/200\n",
      "381/381 [==============================] - 0s 133us/step - loss: 0.2172 - acc: 0.9055\n",
      "Epoch 102/200\n",
      "381/381 [==============================] - 0s 133us/step - loss: 0.2008 - acc: 0.9213\n",
      "Epoch 103/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.2640 - acc: 0.8924\n",
      "Epoch 104/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.2163 - acc: 0.9186\n",
      "Epoch 105/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.2109 - acc: 0.9186\n",
      "Epoch 106/200\n",
      "381/381 [==============================] - 0s 116us/step - loss: 0.2093 - acc: 0.9213\n",
      "Epoch 107/200\n",
      "381/381 [==============================] - 0s 139us/step - loss: 0.2290 - acc: 0.9003\n",
      "Epoch 108/200\n",
      "381/381 [==============================] - 0s 160us/step - loss: 0.2172 - acc: 0.9318\n",
      "Epoch 109/200\n",
      "381/381 [==============================] - 0s 107us/step - loss: 0.2199 - acc: 0.9134\n",
      "Epoch 110/200\n",
      "381/381 [==============================] - 0s 106us/step - loss: 0.2084 - acc: 0.9239\n",
      "Epoch 111/200\n",
      "381/381 [==============================] - 0s 111us/step - loss: 0.2029 - acc: 0.9318\n",
      "Epoch 112/200\n",
      "381/381 [==============================] - 0s 91us/step - loss: 0.1994 - acc: 0.9344\n",
      "Epoch 113/200\n",
      "381/381 [==============================] - 0s 97us/step - loss: 0.2094 - acc: 0.9160\n",
      "Epoch 114/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.1982 - acc: 0.9213\n",
      "Epoch 115/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2110 - acc: 0.9160\n",
      "Epoch 116/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.2187 - acc: 0.9134\n",
      "Epoch 117/200\n",
      "381/381 [==============================] - 0s 123us/step - loss: 0.2777 - acc: 0.9108\n",
      "Epoch 118/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2322 - acc: 0.9213\n",
      "Epoch 119/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.2242 - acc: 0.9160\n",
      "Epoch 120/200\n",
      "381/381 [==============================] - 0s 128us/step - loss: 0.2019 - acc: 0.9239\n",
      "Epoch 121/200\n",
      "381/381 [==============================] - 0s 134us/step - loss: 0.2081 - acc: 0.9265\n",
      "Epoch 122/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2101 - acc: 0.9213\n",
      "Epoch 123/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.1885 - acc: 0.9213\n",
      "Epoch 124/200\n",
      "381/381 [==============================] - 0s 119us/step - loss: 0.1834 - acc: 0.9213\n",
      "Epoch 125/200\n",
      "381/381 [==============================] - 0s 112us/step - loss: 0.2119 - acc: 0.9108\n",
      "Epoch 126/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.1916 - acc: 0.9239\n",
      "Epoch 127/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.2553 - acc: 0.8924\n",
      "Epoch 128/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.1851 - acc: 0.9265\n",
      "Epoch 129/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.1886 - acc: 0.9265\n",
      "Epoch 130/200\n",
      "381/381 [==============================] - 0s 117us/step - loss: 0.1988 - acc: 0.9239\n",
      "Epoch 131/200\n",
      "381/381 [==============================] - 0s 90us/step - loss: 0.1861 - acc: 0.9344\n",
      "Epoch 132/200\n",
      "381/381 [==============================] - 0s 115us/step - loss: 0.2041 - acc: 0.9213\n",
      "Epoch 133/200\n",
      "381/381 [==============================] - 0s 109us/step - loss: 0.2166 - acc: 0.9239\n",
      "Epoch 134/200\n",
      "381/381 [==============================] - 0s 111us/step - loss: 0.1807 - acc: 0.9265\n",
      "Epoch 135/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.1796 - acc: 0.9344\n",
      "Epoch 136/200\n",
      "381/381 [==============================] - 0s 81us/step - loss: 0.1886 - acc: 0.9213\n",
      "Epoch 137/200\n",
      "381/381 [==============================] - 0s 112us/step - loss: 0.2016 - acc: 0.9186\n",
      "Epoch 138/200\n",
      "381/381 [==============================] - 0s 138us/step - loss: 0.1729 - acc: 0.9344\n",
      "Epoch 139/200\n",
      "381/381 [==============================] - 0s 127us/step - loss: 0.1882 - acc: 0.9160\n",
      "Epoch 140/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.1881 - acc: 0.9265\n",
      "Epoch 141/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.1733 - acc: 0.9318\n",
      "Epoch 142/200\n",
      "381/381 [==============================] - 0s 85us/step - loss: 0.1811 - acc: 0.9213\n",
      "Epoch 143/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.1867 - acc: 0.9081\n",
      "Epoch 144/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.2211 - acc: 0.9081\n",
      "Epoch 145/200\n",
      "381/381 [==============================] - 0s 105us/step - loss: 0.1901 - acc: 0.9239\n",
      "Epoch 146/200\n",
      "381/381 [==============================] - 0s 105us/step - loss: 0.1718 - acc: 0.9239\n",
      "Epoch 147/200\n",
      "381/381 [==============================] - 0s 116us/step - loss: 0.1687 - acc: 0.9318\n",
      "Epoch 148/200\n",
      "381/381 [==============================] - 0s 98us/step - loss: 0.1848 - acc: 0.9213\n",
      "Epoch 149/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.1918 - acc: 0.9186\n",
      "Epoch 150/200\n",
      "381/381 [==============================] - 0s 107us/step - loss: 0.1908 - acc: 0.9265\n",
      "Epoch 151/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.1794 - acc: 0.9239\n",
      "Epoch 152/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.1760 - acc: 0.9239\n",
      "Epoch 153/200\n",
      "381/381 [==============================] - 0s 109us/step - loss: 0.1754 - acc: 0.9213\n",
      "Epoch 154/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.1802 - acc: 0.9213\n",
      "Epoch 155/200\n",
      "381/381 [==============================] - 0s 99us/step - loss: 0.1890 - acc: 0.9265\n",
      "Epoch 156/200\n",
      "381/381 [==============================] - 0s 111us/step - loss: 0.1766 - acc: 0.9344\n",
      "Epoch 157/200\n",
      "381/381 [==============================] - 0s 118us/step - loss: 0.2050 - acc: 0.9291\n",
      "Epoch 158/200\n",
      "381/381 [==============================] - 0s 108us/step - loss: 0.1652 - acc: 0.9186\n",
      "Epoch 159/200\n",
      "381/381 [==============================] - 0s 98us/step - loss: 0.1892 - acc: 0.9239\n",
      "Epoch 160/200\n",
      "381/381 [==============================] - 0s 98us/step - loss: 0.1674 - acc: 0.9370\n",
      "Epoch 161/200\n",
      "381/381 [==============================] - 0s 94us/step - loss: 0.2010 - acc: 0.9213\n",
      "Epoch 162/200\n",
      "381/381 [==============================] - 0s 132us/step - loss: 0.2097 - acc: 0.9239\n",
      "Epoch 163/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381/381 [==============================] - 0s 106us/step - loss: 0.1654 - acc: 0.9265\n",
      "Epoch 164/200\n",
      "381/381 [==============================] - 0s 90us/step - loss: 0.1673 - acc: 0.9213\n",
      "Epoch 165/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.1796 - acc: 0.9213\n",
      "Epoch 166/200\n",
      "381/381 [==============================] - 0s 109us/step - loss: 0.1748 - acc: 0.9213\n",
      "Epoch 167/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.1843 - acc: 0.9186\n",
      "Epoch 168/200\n",
      "381/381 [==============================] - 0s 113us/step - loss: 0.1869 - acc: 0.9239\n",
      "Epoch 169/200\n",
      "381/381 [==============================] - 0s 80us/step - loss: 0.1841 - acc: 0.9239\n",
      "Epoch 170/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.1713 - acc: 0.9265\n",
      "Epoch 171/200\n",
      "381/381 [==============================] - 0s 81us/step - loss: 0.1875 - acc: 0.9344\n",
      "Epoch 172/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1614 - acc: 0.9160\n",
      "Epoch 173/200\n",
      "381/381 [==============================] - 0s 89us/step - loss: 0.1684 - acc: 0.9344\n",
      "Epoch 174/200\n",
      "381/381 [==============================] - 0s 91us/step - loss: 0.2210 - acc: 0.9160\n",
      "Epoch 175/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1964 - acc: 0.9213\n",
      "Epoch 176/200\n",
      "381/381 [==============================] - 0s 143us/step - loss: 0.1922 - acc: 0.9291\n",
      "Epoch 177/200\n",
      "381/381 [==============================] - 0s 101us/step - loss: 0.1597 - acc: 0.9423\n",
      "Epoch 178/200\n",
      "381/381 [==============================] - 0s 83us/step - loss: 0.1711 - acc: 0.9265\n",
      "Epoch 179/200\n",
      "381/381 [==============================] - 0s 128us/step - loss: 0.1886 - acc: 0.9265\n",
      "Epoch 180/200\n",
      "381/381 [==============================] - 0s 130us/step - loss: 0.1585 - acc: 0.9291\n",
      "Epoch 181/200\n",
      "381/381 [==============================] - 0s 160us/step - loss: 0.1531 - acc: 0.9344\n",
      "Epoch 182/200\n",
      "381/381 [==============================] - 0s 105us/step - loss: 0.1495 - acc: 0.9554\n",
      "Epoch 183/200\n",
      "381/381 [==============================] - 0s 95us/step - loss: 0.1735 - acc: 0.9134\n",
      "Epoch 184/200\n",
      "381/381 [==============================] - 0s 93us/step - loss: 0.1572 - acc: 0.9370\n",
      "Epoch 185/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1567 - acc: 0.9318\n",
      "Epoch 186/200\n",
      "381/381 [==============================] - 0s 118us/step - loss: 0.1610 - acc: 0.9239\n",
      "Epoch 187/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1622 - acc: 0.9449\n",
      "Epoch 188/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1837 - acc: 0.9265\n",
      "Epoch 189/200\n",
      "381/381 [==============================] - 0s 106us/step - loss: 0.1797 - acc: 0.9265\n",
      "Epoch 190/200\n",
      "381/381 [==============================] - 0s 86us/step - loss: 0.1918 - acc: 0.9239\n",
      "Epoch 191/200\n",
      "381/381 [==============================] - 0s 150us/step - loss: 0.1737 - acc: 0.9265\n",
      "Epoch 192/200\n",
      "381/381 [==============================] - 0s 110us/step - loss: 0.1611 - acc: 0.9291\n",
      "Epoch 193/200\n",
      "381/381 [==============================] - 0s 104us/step - loss: 0.1514 - acc: 0.9344\n",
      "Epoch 194/200\n",
      "381/381 [==============================] - 0s 103us/step - loss: 0.1609 - acc: 0.9265\n",
      "Epoch 195/200\n",
      "381/381 [==============================] - 0s 86us/step - loss: 0.1517 - acc: 0.9370\n",
      "Epoch 196/200\n",
      "381/381 [==============================] - 0s 118us/step - loss: 0.1576 - acc: 0.9291\n",
      "Epoch 197/200\n",
      "381/381 [==============================] - 0s 106us/step - loss: 0.1603 - acc: 0.9291\n",
      "Epoch 198/200\n",
      "381/381 [==============================] - 0s 87us/step - loss: 0.1501 - acc: 0.9475\n",
      "Epoch 199/200\n",
      "381/381 [==============================] - 0s 102us/step - loss: 0.1685 - acc: 0.9265\n",
      "Epoch 200/200\n",
      "381/381 [==============================] - 0s 100us/step - loss: 0.1633 - acc: 0.9370\n"
     ]
    }
   ],
   "source": [
    "trained = model.fit(X_train,Y_train,epochs=200,batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model accuracy : 93.701\n"
     ]
    }
   ],
   "source": [
    "print(\"Trained model accuracy : %2.3f\"%(trained.history['acc'][199]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 211us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test,Y_test,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on testset : 97.340\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on testset : %2.3f\"%(score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9773834  0.0226166 ]\n",
      " [0.0076093  0.9923907 ]\n",
      " [0.03082674 0.9691732 ]\n",
      " [0.00767115 0.9923288 ]\n",
      " [0.07138528 0.92861474]\n",
      " [0.97738886 0.02261113]\n",
      " [0.01379028 0.9862097 ]\n",
      " [0.00679619 0.99320376]\n",
      " [0.04244096 0.95755905]\n",
      " [0.00699733 0.9930027 ]\n",
      " [0.03227131 0.9677287 ]\n",
      " [0.00775761 0.99224246]\n",
      " [0.97738886 0.02261113]\n",
      " [0.01060188 0.9893981 ]\n",
      " [0.00759875 0.99240124]\n",
      " [0.01486979 0.9851302 ]\n",
      " [0.02229629 0.97770375]\n",
      " [0.00701574 0.9929843 ]\n",
      " [0.00938465 0.99061537]\n",
      " [0.03893309 0.9610669 ]\n",
      " [0.18691613 0.8130838 ]\n",
      " [0.00773604 0.9922639 ]\n",
      " [0.04139971 0.95860034]\n",
      " [0.00677767 0.99322236]\n",
      " [0.0069468  0.99305314]\n",
      " [0.45838854 0.54161143]\n",
      " [0.8080282  0.19197173]\n",
      " [0.97738886 0.02261113]\n",
      " [0.02587205 0.9741279 ]\n",
      " [0.97738683 0.02261313]\n",
      " [0.9773909  0.02260914]\n",
      " [0.9773859  0.02261405]\n",
      " [0.2075241  0.79247594]\n",
      " [0.97489434 0.02510572]\n",
      " [0.0066954  0.99330455]\n",
      " [0.0067839  0.9932161 ]\n",
      " [0.9773193  0.02268076]\n",
      " [0.00696642 0.9930335 ]\n",
      " [0.03046342 0.9695366 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.02624173 0.9737583 ]\n",
      " [0.04345659 0.9565434 ]\n",
      " [0.00670156 0.9932985 ]\n",
      " [0.9773882  0.02261181]\n",
      " [0.00705272 0.9929472 ]\n",
      " [0.00679816 0.9932019 ]\n",
      " [0.7021613  0.2978387 ]\n",
      " [0.97738874 0.02261127]\n",
      " [0.0070666  0.9929334 ]\n",
      " [0.99554175 0.00445828]\n",
      " [0.04706994 0.95293003]\n",
      " [0.00669312 0.99330693]\n",
      " [0.03427752 0.96572244]\n",
      " [0.04935851 0.9506415 ]\n",
      " [0.08672773 0.9132723 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.67755467 0.3224453 ]\n",
      " [0.9773767  0.02262337]\n",
      " [0.01413937 0.9858606 ]\n",
      " [0.9771516  0.02284842]\n",
      " [0.97568613 0.02431383]\n",
      " [0.5589493  0.44105065]\n",
      " [0.00736926 0.9926307 ]\n",
      " [0.0073209  0.9926791 ]\n",
      " [0.03994602 0.960054  ]\n",
      " [0.0612111  0.9387889 ]\n",
      " [0.04820778 0.9517922 ]\n",
      " [0.00684949 0.99315053]\n",
      " [0.04250417 0.95749587]\n",
      " [0.7021853  0.29781473]\n",
      " [0.00669937 0.9933007 ]\n",
      " [0.7019684  0.29803154]\n",
      " [0.00669311 0.99330693]\n",
      " [0.01824723 0.98175275]\n",
      " [0.12969328 0.87030673]\n",
      " [0.00766478 0.99233526]\n",
      " [0.34736353 0.65263647]\n",
      " [0.03786254 0.96213746]\n",
      " [0.97738886 0.02261116]\n",
      " [0.5603921  0.43960786]\n",
      " [0.97738886 0.02261113]\n",
      " [0.00708947 0.99291056]\n",
      " [0.9754209  0.02457915]\n",
      " [0.9769363  0.0230637 ]\n",
      " [0.94721466 0.05278531]\n",
      " [0.01437605 0.98562396]\n",
      " [0.97738886 0.02261113]\n",
      " [0.01194894 0.98805106]\n",
      " [0.97738475 0.02261522]\n",
      " [0.97710586 0.02289416]\n",
      " [0.9773886  0.02261137]\n",
      " [0.97738886 0.02261114]\n",
      " [0.03245749 0.9675425 ]\n",
      " [0.04511709 0.9548829 ]\n",
      " [0.97738236 0.02261769]\n",
      " [0.97738886 0.02261113]\n",
      " [0.0066869  0.99331313]\n",
      " [0.02573183 0.9742682 ]\n",
      " [0.00668772 0.9933123 ]\n",
      " [0.00671316 0.9932868 ]\n",
      " [0.00669336 0.9933067 ]\n",
      " [0.97738874 0.0226112 ]\n",
      " [0.07381304 0.926187  ]\n",
      " [0.00835729 0.9916427 ]\n",
      " [0.9746831  0.02531694]\n",
      " [0.97738886 0.02261114]\n",
      " [0.6167693  0.3832307 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.00675048 0.99324954]\n",
      " [0.80238646 0.19761354]\n",
      " [0.00723057 0.9927695 ]\n",
      " [0.97732556 0.02267446]\n",
      " [0.50322443 0.49677554]\n",
      " [0.9767751  0.02322496]\n",
      " [0.01272641 0.9872736 ]\n",
      " [0.05643138 0.9435686 ]\n",
      " [0.70048785 0.2995122 ]\n",
      " [0.0069377  0.9930623 ]\n",
      " [0.7027387  0.2972613 ]\n",
      " [0.0067415  0.9932585 ]\n",
      " [0.03855535 0.9614446 ]\n",
      " [0.03820862 0.9617914 ]\n",
      " [0.9773751  0.02262491]\n",
      " [0.00694293 0.993057  ]\n",
      " [0.9773336  0.02266634]\n",
      " [0.00669253 0.99330753]\n",
      " [0.97738886 0.02261113]\n",
      " [0.01309681 0.9869032 ]\n",
      " [0.04153213 0.9584679 ]\n",
      " [0.98949414 0.01050585]\n",
      " [0.01575209 0.9842479 ]\n",
      " [0.00832521 0.99167484]\n",
      " [0.0067091  0.9932909 ]\n",
      " [0.02572887 0.9742711 ]\n",
      " [0.08506738 0.9149326 ]\n",
      " [0.975255   0.02474506]\n",
      " [0.04192654 0.9580735 ]\n",
      " [0.00713035 0.9928697 ]\n",
      " [0.03431389 0.9656861 ]\n",
      " [0.00668823 0.99331176]\n",
      " [0.04551877 0.9544812 ]\n",
      " [0.13035451 0.8696455 ]\n",
      " [0.03746587 0.9625341 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.00677366 0.99322635]\n",
      " [0.0169393  0.98306066]\n",
      " [0.00668777 0.9933122 ]\n",
      " [0.97738886 0.02261114]\n",
      " [0.05780043 0.9421995 ]\n",
      " [0.00670657 0.9932934 ]\n",
      " [0.00676231 0.9932376 ]\n",
      " [0.9447075  0.05529246]\n",
      " [0.00711833 0.99288166]\n",
      " [0.02853349 0.9714665 ]\n",
      " [0.00681223 0.9931878 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.00670367 0.9932963 ]\n",
      " [0.09090226 0.9090977 ]\n",
      " [0.04344756 0.95655245]\n",
      " [0.04972066 0.95027936]\n",
      " [0.00678774 0.9932122 ]\n",
      " [0.00700652 0.9929934 ]\n",
      " [0.00668716 0.9933128 ]\n",
      " [0.00674545 0.9932546 ]\n",
      " [0.97738886 0.02261113]\n",
      " [0.97738886 0.02261114]\n",
      " [0.04742079 0.95257926]\n",
      " [0.01436054 0.98563945]\n",
      " [0.15559343 0.8444066 ]\n",
      " [0.00673853 0.9932615 ]\n",
      " [0.01081068 0.98918927]\n",
      " [0.8286679  0.17133215]\n",
      " [0.00905141 0.9909486 ]\n",
      " [0.9281069  0.0718931 ]\n",
      " [0.0066856  0.99331445]\n",
      " [0.03722437 0.9627756 ]\n",
      " [0.00668709 0.9933129 ]\n",
      " [0.00867662 0.99132335]\n",
      " [0.00669144 0.99330854]\n",
      " [0.9773875  0.02261253]\n",
      " [0.00796183 0.99203813]\n",
      " [0.00706372 0.9929363 ]\n",
      " [0.00678096 0.9932191 ]\n",
      " [0.97585344 0.02414659]\n",
      " [0.016835   0.983165  ]\n",
      " [0.97738826 0.02261172]\n",
      " [0.00670231 0.99329776]\n",
      " [0.04471821 0.95528173]]\n",
      "1st prediction : 0.9773834 0.0226166\n",
      "1.000\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test)\n",
    "print(prediction)\n",
    "\n",
    "#Printing result for first prediction\n",
    "print(\"1st prediction : \"+str(prediction[0][0])+\" \"+str(prediction[0][1]))\n",
    "print(\"%1.3f\"%sum(prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
